{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-gramy słów w klasyfikacji na przykładzie SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SO0jQYNKklfw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W słowniku znajduje się 287718 różnych cech\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.94      0.91       389\n",
      "           1       0.92      0.94      0.93       385\n",
      "           2       0.95      0.90      0.92       394\n",
      "           3       0.97      0.90      0.93       310\n",
      "\n",
      "    accuracy                           0.92      1478\n",
      "   macro avg       0.93      0.92      0.92      1478\n",
      "weighted avg       0.92      0.92      0.92      1478\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.datasets import fetch_20newsgroups # zbiór danych zawarty w Sklearn, który zawiera dane z 20 grup newsowych\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "np.random.seed(0) # ustawienie seed na 0\n",
    "\n",
    "categories = ['sci.space', 'comp.graphics', 'talk.politics.misc', 'comp.sys.mac.hardware'] # kategorie do analizy\n",
    "\n",
    "train = fetch_20newsgroups(subset='train',\n",
    "                                   categories=categories,\n",
    "                                   shuffle=True,\n",
    "                                   random_state=42) # pobranie zbioru treningowego\n",
    "    \n",
    "\n",
    "test = fetch_20newsgroups(subset='test',\n",
    "                                  categories=categories,\n",
    "                                  shuffle=True,\n",
    "                                  random_state=42) # pobranie zbioru testowego\n",
    "\n",
    "\n",
    "\n",
    "pipeline = Pipeline([             # stworzenie pipeline'u: surowy tekst -> TFIDF vectorizer -> klasyfikator \n",
    "    ('tfidf', TfidfVectorizer(max_df=0.1, ngram_range=(1,2))),\n",
    "    ('clf', SVC(C=1.0, kernel='linear')),\n",
    "])\n",
    "\n",
    "\n",
    "pipeline.fit(train.data, train.target) # wektoryzacja danych i trenowanie klasyfikatora na zbiorze treningowym\n",
    "\n",
    "print(\"W słowniku znajduje się {n} różnych cech\".format(\n",
    "    n=len(pipeline.named_steps['tfidf'].vocabulary_.keys())))\n",
    "\n",
    "\n",
    "print(classification_report(test.target, pipeline.predict(test.data))) # testowanie klasyfikatora - szerokie podsumowanie uwzględniające miary: precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wykorzystanie n-gramów w klasyfikacji - detekcja języka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ULRDlJwzklfz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oto kilka przykładowych cech stworzonych przez TfidfVectorizer: ['ap', 'pe', 'el', 'lu', 'je']\n",
      "\n",
      "\n",
      "Tekst: Bonjour! został zaklasyfikowany jako: french\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     english       1.00      1.00      1.00       303\n",
      "      french       1.00      1.00      1.00       280\n",
      "      german       1.00      1.00      1.00       337\n",
      "     italian       1.00      1.00      1.00       273\n",
      "      polish       1.00      1.00      1.00       291\n",
      "     spanish       1.00      1.00      1.00       299\n",
      "\n",
      "    accuracy                           1.00      1783\n",
      "   macro avg       1.00      1.00      1.00      1783\n",
      "weighted avg       1.00      1.00      1.00      1783\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_class_name_from_id(ids, mapping): # funkcja mapująca identyfikator liczbowy kategorii na wartość tekstową, np: 0->\"polish\", 1->\"english\"\n",
    "    return [mapping[id] for id in ids]\n",
    "\n",
    "\n",
    "full_dataset = pandas.read_csv('language_detection_1000.csv', encoding='utf-8') # wczytanie danych z pliku CSV\n",
    "lang_to_id = {'polish': 0, 'english': 1, 'french': 2,\n",
    "              'german': 3, 'italian': 4, 'spanish': 5}\n",
    "id_to_lang = {v: k for k,v in lang_to_id.items()}\n",
    "full_dataset['label_num'] = full_dataset.lang.map(lang_to_id)  # mapowanie wartości na liczby \n",
    "\n",
    "np.random.seed(0)                                       # ustawienie seed na 0\n",
    "train_indices = np.random.rand(len(full_dataset)) < 0.7 # wylosowanie 70% wierszy, które znajdą się w zbiorze treningowym\n",
    "\n",
    "train = full_dataset[train_indices] # wybranie zbioru treningowego\n",
    "test = full_dataset[~train_indices] # wybranie zbioru testowego\n",
    "\n",
    "\n",
    "pipeline = Pipeline([             # stworzenie pipeline'u: surowy tekst -> TFIDF vectorizer -> klasyfikator \n",
    "    ('tfidf', TfidfVectorizer(max_features=300, ngram_range=(2,2), analyzer=\"char\")),\n",
    "    ('scaler', StandardScaler(with_mean = False)),\n",
    "    ('clf', LogisticRegression()),\n",
    "])\n",
    "\n",
    "\n",
    "pipeline.fit(train['text'], train['label_num']) # wektoryzacja danych i trening klasyfikatora\n",
    "\n",
    "print(\"Oto kilka przykładowych cech stworzonych przez TfidfVectorizer: {n}\".format(\n",
    "    n=list(pipeline.named_steps['tfidf'].vocabulary_.keys())[:5]))\n",
    "\n",
    "\n",
    "text_to_predict = \"Bonjour!\" # sprawdzenie działania\n",
    "predicted = pipeline.predict([text_to_predict])\n",
    "print(\"\\n\\nTekst: {t} został zaklasyfikowany jako: {p}\\n\\n\".format(\n",
    "    t=text_to_predict,\n",
    "    p=id_to_lang[predicted[0]]\n",
    "))\n",
    "\n",
    "\n",
    "print(classification_report( # ocena klasyfikatora\n",
    "    get_class_name_from_id(test['label_num'], id_to_lang), \n",
    "    get_class_name_from_id(pipeline.predict(test['text']), id_to_lang)\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sprawdzenie istotności cech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1F01zfofklf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W rozpoznaniu języka polish najważniejsze cechy to:\n",
      "\t'dz': 0.19163713727725623\n",
      "\t'cz': 0.1649009131198807\n",
      "\t'ię': 0.1640251859302473\n",
      "\t'ow': 0.16229669569677055\n",
      "\t'wa': 0.15187384680981408\n",
      "W rozpoznaniu języka english najważniejsze cechy to:\n",
      "\t'th': 0.42383859836240256\n",
      "\t' t': 0.37872218470723895\n",
      "\t'd ': 0.28129480440165094\n",
      "\t'of': 0.25858398649029607\n",
      "\t'y ': 0.2520002950043387\n",
      "W rozpoznaniu języka french najważniejsze cechy to:\n",
      "\t't ': 0.278915327225123\n",
      "\t'té': 0.26532601045539983\n",
      "\t'ré': 0.24798957436559332\n",
      "\t'ai': 0.24521470119052077\n",
      "\t'ce': 0.23995343713432304\n",
      "W rozpoznaniu języka german najważniejsze cechy to:\n",
      "\t'ei': 0.25373304796517443\n",
      "\t'au': 0.23734904149545072\n",
      "\t'er': 0.23458829477837226\n",
      "\t'ch': 0.23439514106726453\n",
      "\t'en': 0.2275646227666411\n",
      "W rozpoznaniu języka italian najważniejsze cechy to:\n",
      "\t'i ': 0.4288409677099189\n",
      "\t'o ': 0.31534222725009353\n",
      "\t'zi': 0.28110382653414995\n",
      "\t'tt': 0.27269593895744765\n",
      "\t'll': 0.27156186429069823\n",
      "W rozpoznaniu języka spanish najważniejsze cechy to:\n",
      "\t'os': 0.3000944777602326\n",
      "\t'ió': 0.29055515786258945\n",
      "\t'ón': 0.28997134551825343\n",
      "\t'ci': 0.2666069233599421\n",
      "\t' y': 0.2653283725635646\n"
     ]
    }
   ],
   "source": [
    "def language_indicators(feature_names, feature_importances, id_to_lang): # funkcja oceniająca cechy najsilniej skojarzone z klasami\n",
    "    for i, language in enumerate(feature_importances): # iteracja po macierzy feature_importances (wymiarów: język x cechy) wierszami (język po języku)\n",
    "        scored_features = list(zip(feature_names, language)) # tworzenie skojarzenia nazw cech z wagami modelu (ponieważ używamy regresji logistycznej - każda cecha ma swoją wagę, która jest optymalizowana w procesie uczenia. Każda klasa ma osobny model ze swoimi wagami)\n",
    "        scored_features = sorted(scored_features, key=lambda x: x[1], reverse=True) # sortowanie cech skojarzonych z wagami malejąco \n",
    "        print(\"W rozpoznaniu języka {lang} najważniejsze cechy to:\".format(\n",
    "            lang=id_to_lang[i]) # zamiana identyfikatora na nazwę języka\n",
    "        )\n",
    "        for feature, score in scored_features[:5]: # wybór 5 najważniejszych cech\n",
    "            print(\"\\t'{feature}': {score}\".format(feature=feature, score=score))\n",
    "        \n",
    "\n",
    "language_indicators( # wyświetlenie najważniejszych cech dla każdej kategorii\n",
    "    pipeline.named_steps['tfidf'].get_feature_names_out(), # pobranie nazwy cechy\n",
    "    pipeline.named_steps['clf'].coef_, # pobranie wyuczonych współczynnikiów \n",
    "    id_to_lang # mapowanie z identyfikatora numerycznego na pełną nazwę języka\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wykorzystanie n-gramów słów w generowaniu tekstu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ooeUA4Gbklf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'big', 'brown'], ['big', 'brown', 'fox'], ['brown', 'fox', 'jumped'], ['fox', 'jumped', 'over'], ['jumped', 'over', 'the'], ['over', 'the', 'fence.']]\n"
     ]
    }
   ],
   "source": [
    "def get_word_ngrams(data, n_gram_len): # funkcja dzieląca tekst na n-gramy słów \n",
    "    splitted = data.split()\n",
    "    result = []\n",
    "    for i in range(len(splitted) - n_gram_len + 1):\n",
    "        tmp = []\n",
    "        for j in range(n_gram_len):\n",
    "            tmp.append(splitted[i+j])\n",
    "        result.append(tmp)\n",
    "    return result\n",
    "print(get_word_ngrams(\"The big brown fox jumped over the fence.\", 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generowanie tekstu metodą Markova korzystając z n-gramów słów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2GrZxjouklf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Średnio co dwa miesiące od początku roku - wskutek opóźnienia projektów inwestycyjnych, zwłaszcza nabucco i innych krajów. zasadniczo istnieje wysokie ryzyko omijania przepisów krajowych, ma ogromne znaczenie w tych negocjacjach, która cechuje się delikatnością i stanowi zagrożenie dla gospodarki. niestety, bezrobocie w młodym wieku ma długotrwałe i negatywne skutki. niestety, chociaż jest ona potrzebna. ue udowodniła to raz jeszcze, niezwłocznie wypłacając środki pomocowe w krajach rozwijających się. dla obywateli ze wschodu i z wynikami odnośnych ocen rocznych. w tym konkretnym przypadku, ale chciałbym dotknąć dwóch rzeczy. ze zbliżającym się przyjęciem konwencji mop w sprawie sudanu, koncentrując się głównie na stworzeniu środowiska, w którym zauważa, że wciąż zdarza się, iż takie właśnie stanowisko chcemy zająć. musimy jednak wykazać poczucie odpowiedzialności wśród zainteresowanych stron powinien przyczynić się do wzmożonego ataku na sektor publiczny; zawiera ono szereg środków, które przedstawiła w charakterze klastrów, sieci krajowych lub regionalnych platform technologicznych jest zadaniem niezbędnym. umożliwią one dostęp do kluczowych usług użyteczności publicznej. chcemy zatem zagwarantować udostępnienie finansowania we właściwym czasie. głosowałem \"za”. bardzo doceniam też wybitną rolę, jaką unia europejska jest drugim co do wielkości gospodarką na szczeblu regionalnym, krajowym i regionalnym. musimy być w stanie osiągnąć celów. podlegają one również naruszane w niektórych sektorach można osiągnąć jedynie wówczas, jeżeli będzie to filar wzrostu gospodarczego i dalszego pogarszania sytuacji w czasach, gdy nie znamy pochodzenia ubrań noszonych przez konsumentów. pani poseł mccarthy, która wzywa komisję do przeprowadzenia studium w tej sprawie, do corocznego opracowywania sprawozdań na temat portugalii i hiszpanii, gdy tymczasem silniejsze, bogatsze państwa jeszcze nie tylko częściej niż osoby dorosłe pozostają bez pracy, a także dla podatników, którzy - całkiem słusznie proponuje przeprowadzenie w 2009 roku wyniosła 24 000 000 euro i podatki wrzuca się do omówienia z radą i komisją w procesie programowania. oczekuję, że trybunał obrachunkowy stwierdził, iż uzyskał wystarczającą pewność, że sprawozdanie roczne za rok finansowy 2009 jest wiarygodne, a transakcje leżące u jego podstaw są zgodne z prawem. istnienie takiego pułapu byłoby koszmarem pod względem standardów regulacyjnych; wpisana w ramy prawne postępowania z wypalonym paliwem jądrowym i odpadami promieniotwórczymi, od ich zwrócenia. nowych, neutralnych i sprawiedliwych kryteriów obliczania płatności bezpośrednich, wypłacania specjalnej pomocy drobnym rolnikom oraz udzielania pomocy wyłącznie rolnikom prowadzącym aktywną działalność oraz na konkluzje rady i planu działania. w związku z wypowiedzią wygłoszoną w programie \"péntek 8 mondatvadász” 23 stycznia 2004 r.) decyzji ramowej parlament europejski będzie wspierać komisję w dniu 22 grudnia 2007 r., kiedy przypada europejski dzień wiedzy o dziedzictwie kulturowym wśród obywateli jest dobrym sposobem na zwiększenie środków. to pozytywny wniosek, do którego część państw członkowskich w latach 2007-2013 na rozwój wsi, dlatego że nasze stanowisko wobec hamasu się zmienił. chciałbym zaapelować do pani, wzywając do stworzenia specjalnego systemu, według którego ue i jej studentom za ich przeznaczenie. po pierwsze, w przypadku naszego zachowania w przyszłości traktowany poważnie przez wyborców, musi jednoznacznie i bezkompromisowo stanąć w obronie prawa posłów i wszystkich osób oraz poważnego uszczerbku na zdrowiu tysięcy pracowników i ludności. obawa, że coś przebiega nieprawidłowo, żebyśmy na koniec chciałbym wspomnieć, że ta sprawa musi nas zmusić\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "def generate_ngram_markov(n_gram_len):\n",
    "    markov_dict = dict() # stworzenie słownika, który wskaże listę dozwolonych słów po zaobserwowanych n-poprzednich słowach.\n",
    "    with open(\"polish_europarl.txt\", 'r', encoding='utf8') as f: # wczytanie korpusu danych\n",
    "        data = f.read().lower()                                  # zamiana wszystkich wielkich liter na małe\n",
    "        n_grams = get_word_ngrams(data, n_gram_len)              # wygenerowanie wszystkich n-gramów słów z korpusu\n",
    "        for n_gram in n_grams:\n",
    "            context = \" \".join(n_gram[:-1])      # połączenie wszystkich słów z n-gramu poza ostatnim w 1 string\n",
    "            last_word = str(n_gram[-1])\n",
    "            \n",
    "            if context not in markov_dict.keys(): # jeśli n-gram bez ostatniego słowa nie występuje w słowniku,\n",
    "                markov_dict[context] = list()     # należy dopisać go do słownika i stworzyć mu listę\n",
    "            markov_dict[context].append(last_word) # wiedząc, że ubiedzony n-gram jest w słowniku, należy dopisać ostatnie słowo do listy\n",
    "    \n",
    "    for context in markov_dict.keys():\n",
    "        markov_dict[context] = Counter(markov_dict[context])  # stworzenie histogramu słów jakie występują w korpusie po kontekście\n",
    "    \n",
    "    return markov_dict\n",
    "\n",
    "\n",
    "n_gram_len = 3  # liczba słów do stworznia n-gramu\n",
    "markov_dict = generate_ngram_markov(n_gram_len)  # stworzenie słownika z histogramami słów dla poszczególnych kontekstów\n",
    "\n",
    "text = 'Średnio co dwa' # tekst, od którego rozpocznie się generowanie\n",
    "\n",
    "for i in range(500):\n",
    "    text_spl = text.split(\" \")     # podzielenie istniejącego tekstu po spacji\n",
    "    context = \" \".join(text_spl[-n_gram_len+1:])   # pobranie ostatnich n_gram_len - 1 słów\n",
    "    idx = random.randrange(sum(markov_dict[context].values())) # sprawdenie dozwolonych słów jako następniki kontekstu (context) i wybór następnika, który zostanie wylosowany zgodnie z rozkładem stworzonym przez histogram\n",
    "    new_word = next(itertools.islice(markov_dict[context].elements(), idx, None)) # wybranie wylosowanego słowa\n",
    "    text = text + \" \" + new_word # doklejenie wylosowanego słowa\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wykorzystanie n-gramów znaków w generowaniu tekstu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Egh-HRhaklf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'he ', 'e b', ' bi', 'big', 'ig ', 'g b', ' br', 'bro', 'row', 'own', 'wn ', 'n f', ' fo', 'fox', 'ox ', 'x j', ' ju', 'jum', 'ump', 'mpe', 'ped', 'ed ', 'd o', ' ov', 'ove', 'ver', 'er ', 'r t', ' th', 'the', 'he ', 'e f', ' fe', 'fen', 'enc', 'nce', 'ce.']\n"
     ]
    }
   ],
   "source": [
    "def get_character_ngrams(data, n_gram_len): # funkcja dzieląca tekst na n-gramy znaków\n",
    "    result = []\n",
    "    for i in range(len(data) - n_gram_len + 1):\n",
    "        result.append(data[i:i+n_gram_len])\n",
    "    return result\n",
    "print(get_character_ngrams(\"The big brown fox jumped over the fence.\", 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generowanie tekstu metodą Markova korzystając z n-gramów znaków"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qMzT3HLaklf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U szlachty tłuszcza;\n",
      "bo pan bóg, kiedy karę na naród przypuszcza,\n",
      "zmykali. zwierz zwraca się czasem,\n",
      "spojrzy, klapnie paszczęki na nogach się opierał ryków — ot słowo! co po waszej zgubie?\n",
      "ja człek podeszły w lata,\n",
      "w podróżach swych zbawców zobaczy.\n",
      "sędzia poznał: «jak się masz, mój jaśnie wielmożnej tytuł przybrać miała,\n",
      "a znów tylko brzęk usłyszy i rymów porządek, lecz go tam nie rozpusta płocha,\n",
      "lecz mniej wielkie, mniej pilni.\n",
      "\n",
      "    tymczasem buchman radości, niestety, sprzątniono…\n",
      "a i to, bóg mi świad\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "def generate_ngram_markov(n_gram_len):\n",
    "    markov_dict = dict() # stworzenie słownik, który wskaże listę dozwolonych słów po zaobserwowanych n-poprzednich słowach\n",
    "    with open('pan_tadeusz.txt', 'r', encoding='utf8') as f: # wczytanie korpusu danych\n",
    "        data = f.read().lower()                              # zamiana wszystkich wielkich liter na małe\n",
    "        n_grams = get_character_ngrams(data, n_gram_len)     # wygenerowanie wszystkich n-gramów słów z korpusu\n",
    "        for n_gram in n_grams:\n",
    "            context = n_gram[:-1]\n",
    "            last_char = n_gram[-1]\n",
    "            if context not in markov_dict.keys(): # jeśli n-gram bez ostatniego znaku nie występuje jeszcze w słowniku,\n",
    "                markov_dict[context] = list()     # należy dopisać go do słownika i stwórzyć mu listę\n",
    "            markov_dict[context].append(last_char) # wiedząc, że ubiedzony n-gram jest w słowniku, należy dopisać ostatni znak do listy\n",
    "    \n",
    "    for context in markov_dict.keys():\n",
    "        markov_dict[context] = Counter(markov_dict[context])  # stworzenie histogramu liter jakie występują w korpusie po kontekście\n",
    "    \n",
    "    return markov_dict\n",
    "\n",
    "\n",
    "text = 'U szlachty' # tekst, od którego rozpocznie się generowanie\n",
    "n_gram_len = len(text)  # liczba znaków do stworznia n-gramu\n",
    "markov_dict = generate_ngram_markov(n_gram_len)  # stworzenie słownika z histogramami słów dla poszczególnych kontekstów\n",
    "\n",
    "for i in range(500):\n",
    "    context = text[-n_gram_len+1:]   # pobranie ostatnich n_gram_len - 1 słów\n",
    "    idx = random.randrange(sum(markov_dict[context].values())) # sprawdzenie dozwolonych słów jako następniki kontekstu i wybór takiego, który zostanie wylosowany zgodnie z rozkładem stworzonym przez histogram\n",
    "    new_char = next(itertools.islice(markov_dict[context].elements(), idx, None)) # wybranie wylosowanego słowa\n",
    "    text = text + new_char # doklejenie wylosowanego znaku na końcu\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4zrSMJRJklf6"
   },
   "outputs": [],
   "source": [
    "# Zad 5:\n",
    "#  Pytanie 1:  Przy zbyt krótkich n-gramach występuje ryzyko stworzenia tekstu, którego kolejne słowa słowa lub głoski bedą\n",
    "# wydawały się losowe.\n",
    "#  Pytanie 2:  Przy zbyt długich n-gramach występuje ryzyko stworzenia tekstu, który będzie bardzo podobny do tekstu \n",
    "# oryginalnego."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Zadania_lab3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
