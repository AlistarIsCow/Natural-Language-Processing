{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sieci rekurencyjne (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicjalizacja wag sieci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.5488135  -0.71518937  0.60276338]\n",
      " [ 0.54488318  0.4236548  -0.64589411]\n",
      " [-0.43758721 -0.891773   -0.96366276]\n",
      " [-0.38344152  0.79172504 -0.52889492]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "def generate_random_matrix(rows, cols): # funkcja generująca macierz o wartościach <-1, 1> o zadanych rozmiarach\n",
    "    x = np.random.rand(rows, cols)\n",
    "    y = np.random.rand(rows, cols)\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            if (y[i][j] > 0.5):\n",
    "                x[i][j] *= -1\n",
    "    return x\n",
    "\n",
    "print(generate_random_matrix(4, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wyznaczenie aktywacji warstwy ukrytej (pamięci) i wyjściowej\n",
    "\n",
    "W przypadku sieci rekurencyjnych pojawia się połączenie rekursywne z historycznymi danymi. \n",
    "Dla prostej rekurencyjnej sieci neuronowej z jedną warstwą ukrytą, obliczenia będą wyglądać następująco:\n",
    "\n",
    "$\\vec{a_{h1(t)}} = sigmoid(U\\vec{x} + W\\vec{a_{h1(t-1)}} + \\vec{b_1})$ - aktywacja f. sigmoid jest tylko przykładem,  można użyć innej aktywacji (tanh, relu, etc)\n",
    "\n",
    "$\\vec{a_{o(t)}} = softmax(V\\vec{a_{h1(t)}} + \\vec{b_2})$ - softmax jest przykładem f. aktywacji, ma sens w problemie klasyfikacji przy > 2 etykietach, można również użyć np. sigmoid (jeśli jeden neuron na wyjściu)\n",
    "\n",
    "<br/>\n",
    "\n",
    "gdzie $\\vec{a_{h1(t)}}$ to wartość wektora reprezentującego aktywację warstwy ukrytej w aktualnym kroku,\n",
    "\n",
    "$\\vec{a_{h1(t-1)}}$ to wartość aktywacji warstwy ukrytej w poprzednim kroku,\n",
    "\n",
    "$\\vec{a_{o(t)}}$ to rezultat wygenerowany przez całą sieć neuronową (aktywacja na warstwie \"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.37191738 0.97551727 0.36888573]\n",
      "[0.37811473 0.33866203 0.28322324]\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x): \n",
    "    output = 1 / (1 + np.exp(-x))\n",
    "    return output\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "U = np.array([[-3., 2., ], [4., 2., ], [1., -5.,]])\n",
    "W = np.array([[1.25, 1.3, 1.5], [2.01, 3.4, -2.4], [1.08, -.3, 0.1]])\n",
    "V = np.array([[-0.2,0.81, -0.2], [0.12, 0.42, 0.21], [0.1, 0.32, 0.01]])\n",
    "\n",
    "x = np.array([0.5, 0.21])\n",
    "prev_hidden = np.array([0.1, 0.32, 0.01])\n",
    "\n",
    "def get_hidden_state_activation(U, W, x, prev_hidden): # funkcja obliczająca aktualną wartość wektora w warstwie ukrytej (pamięć o sekwencji)\n",
    "    vector = U.dot(x) + W.dot(prev_hidden)\n",
    "    output = sigmoid(vector)\n",
    "    return output\n",
    "\n",
    "def get_network_output(V, current_hidden): # funkcja obliczająca wyjście sieci dla aktualnie obliczonych wartości w warstwie ukrytej\n",
    "    vector = V.dot(current_hidden)\n",
    "    output = softmax(vector)\n",
    "    return output\n",
    "\n",
    "\n",
    "hidden_state = get_hidden_state_activation(U, W, x, prev_hidden)\n",
    "print(hidden_state)\n",
    "print(get_network_output(V, hidden_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Przykładowa implementacja sieci neuronowej dodająca do siebie dwie liczby binarne (many-to-many)\n",
    "Dla uproszczenia liczby wejściowe i wyjściowe są tej samej długości"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Błąd przykładu:[4.02600273]\n",
      "Przewidziana sekwencja:[1 1 1 0 0 1 0 0]\n",
      "Oczekiwana sekwencja:  [0 1 0 0 0 1 0 1]\n",
      "------------\n",
      "Błąd przykładu:[1.50900826]\n",
      "Przewidziana sekwencja:[1 1 0 0 0 0 1 0]\n",
      "Oczekiwana sekwencja:  [1 1 0 0 0 0 1 0]\n",
      "------------\n",
      "Błąd przykładu:[0.32491466]\n",
      "Przewidziana sekwencja:[0 1 0 1 1 1 1 1]\n",
      "Oczekiwana sekwencja:  [0 1 0 1 1 1 1 1]\n",
      "------------\n",
      "Błąd przykładu:[0.16574001]\n",
      "Przewidziana sekwencja:[0 1 1 0 1 0 0 1]\n",
      "Oczekiwana sekwencja:  [0 1 1 0 1 0 0 1]\n",
      "------------\n",
      "Błąd przykładu:[0.21679244]\n",
      "Przewidziana sekwencja:[1 0 0 1 1 0 1 1]\n",
      "Oczekiwana sekwencja:  [1 0 0 1 1 0 1 1]\n",
      "------------\n",
      "Błąd przykładu:[0.15873883]\n",
      "Przewidziana sekwencja:[0 1 0 0 0 1 0 1]\n",
      "Oczekiwana sekwencja:  [0 1 0 0 0 1 0 1]\n",
      "------------\n",
      "Błąd przykładu:[0.15120312]\n",
      "Przewidziana sekwencja:[1 0 1 0 1 0 1 0]\n",
      "Oczekiwana sekwencja:  [1 0 1 0 1 0 1 0]\n",
      "------------\n",
      "Błąd przykładu:[0.15645891]\n",
      "Przewidziana sekwencja:[1 0 1 0 1 1 1 1]\n",
      "Oczekiwana sekwencja:  [1 0 1 0 1 1 1 1]\n",
      "------------\n",
      "Błąd przykładu:[0.10274103]\n",
      "Przewidziana sekwencja:[0 1 0 1 0 0 0 0]\n",
      "Oczekiwana sekwencja:  [0 1 0 1 0 0 0 0]\n",
      "------------\n",
      "Błąd przykładu:[0.09490627]\n",
      "Przewidziana sekwencja:[0 1 0 1 1 0 1 1]\n",
      "Oczekiwana sekwencja:  [0 1 0 1 1 0 1 1]\n",
      "------------\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "def sigmoid(x): \n",
    "    output = 1 / (1 + np.exp(-x))\n",
    "    return output\n",
    "\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output * (1 - output)\n",
    "\n",
    "def load_dataset(path): #wczytanie danych wejściowych w formacie liczba1_binarnie,liczba2_binarnie,ich_suma_binarnie\n",
    "    X = []  # lista par składników\n",
    "    Y = []  # lista sum składników\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            if len(line.strip()) == 0:\n",
    "                continue\n",
    "            input1_bin, input2_bin, sum_bin = line.strip().split(\",\")\n",
    "            \n",
    "            input1_bin = np.array([int(i) for i in input1_bin]) # przekształcenie liczb na wektory\n",
    "            input2_bin = np.array([int(i) for i in input2_bin]) # przekształcenie liczb na wektory\n",
    "            sum_bin = np.array([int(i) for i in sum_bin])\n",
    "            \n",
    "            X.append((input1_bin, input2_bin)) # zapisanie par składników jako wejścia\n",
    "            Y.append(sum_bin)                  # zapisanie sumy jako oczekiwany rezultat\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "train_X, train_Y = load_dataset('dataset.csv')\n",
    "\n",
    "alpha = 0.1      # stała uczenia (learning_rate) - jak duże zmiany robić w uczeniu\n",
    "input_dim = 2    # liczba cech (liczb) na wejściu sieci\n",
    "hidden_dim = 16  # rozmiar warstwy ukrytej (rozmiar wektora z pamięcią)\n",
    "output_dim = 1   # liczba wartości na wyjściu \n",
    "\n",
    "U = generate_random_matrix(input_dim, hidden_dim)   # inicjalizacja macierzy między wejściem a warstwą ukrytą\n",
    "V = generate_random_matrix(hidden_dim, output_dim)  # inicjalizacja macierzy między warstwą ukrytą a wyjściową\n",
    "W = generate_random_matrix(hidden_dim, hidden_dim)  # inicjalizacja macierzy między poprzednim stanem warstwy ukrytej a aktualnym\n",
    "\n",
    "U_update = np.zeros_like(U) # macierz poprawek, które aplikowane są w procesie uczenia, aby wyznaczyć coraz lepsze wartości wag\n",
    "V_update = np.zeros_like(V) # macierz poprawek, które aplikowane są w procesie uczenia, aby wyznaczyć coraz lepsze wartości wag\n",
    "W_update = np.zeros_like(W) # macierz poprawek, które aplikowane są w procesie uczenia, aby wyznaczyć coraz lepsze wartości wag\n",
    "\n",
    "for j in range(len(train_Y)):    # iteracja po wszystkich przykładach uczących\n",
    "    added_one_seq = train_X[j][0]   # pierwszy składnik do sumowania w postaci binarnej, np. 00001\n",
    "    added_two_seq = train_X[j][1]   # drugi składnik do sumowania w postaci binarnej, np. 00010\n",
    "    expected_sum_seq = train_Y[j]   # oczekiwany wynik sumowania obu składników (dla przykładu wyżej: 00011)\n",
    "\n",
    "    predicted_sum_seq = np.zeros_like(expected_sum_seq) # pusty wektor, który wypełniany będzie wartościami 0 lub 1 tworząc predykcję\n",
    "\n",
    "    overallError = 0                # różnica przewidywań od wartości oczekiwanej  \n",
    "\n",
    "    output_l_deltas = list()\n",
    "    hidden_l_values = list()\n",
    "    hidden_l_values.append(np.zeros(hidden_dim))\n",
    "\n",
    "    # iteracja po postaci binarnej bit po bicie, od najmniej znaczącego (od prawej do lewej)\n",
    "    for position in range(len(added_one_seq) - 1, -1, -1):\n",
    "\n",
    "        # jako wejście sieci w aktualnym kroku brana jest para bitów na pozycji [position] (2 liczby)\n",
    "        X = np.array([\n",
    "            [added_one_seq[position], added_two_seq[position]]\n",
    "        ])\n",
    "        # jako oczekiwane wyjście sieci w aktualnym kroku brany jest bit na pozycji [position] (odpowiedź budowana jest jako sekwencja, znak po znaku)\n",
    "        y = np.array([[expected_sum_seq[position]]]).T\n",
    "\n",
    "        # obliczenie wartość warstwy ukrytej\n",
    "        hidden_l = get_hidden_state_activation(X, hidden_l_values[-1], U, W)\n",
    "        \n",
    "        # pobranie wygenerowanego wyjścia sieci neuronowej (sigmoida, a nie softmax, gdyż jest tylko jedno wyjście binarne)\n",
    "        predicted_char = sigmoid(np.dot(hidden_l, V))\n",
    "\n",
    "        # sprawdzenie błędu\n",
    "        output_l_error = predicted_char - y\n",
    "\n",
    "        output_l_deltas.append(\n",
    "            (output_l_error) * sigmoid_output_to_derivative(predicted_char)   # loss * grad out\n",
    "        )\n",
    "        overallError += np.abs(output_l_error[0]) # zwiększenie całościowego błędu o błąd z aktualnego kroku\n",
    "\n",
    "        predicted_sum_seq[position] = np.round(predicted_char[0][0]) # zapisanie przewidywania na odpowiedniej pozycji, zaokrąglając (predicted_char to wartość rzeczywista między 0 a 1, zawiera prawdopodobieństwo tego, że liczba powinna być 1-nką) \n",
    "\n",
    "        hidden_l_values.append(copy.deepcopy(hidden_l)) # zapisanie wartości aktywacji warstwy ukrytej, aby można było ją użyć w kolejnym kroku\n",
    "\n",
    "    future_hidden_l_delta = np.zeros(hidden_dim)        \n",
    "    \n",
    "    # propagacja wsteczna \n",
    "    for position in range(len(added_one_seq)):              \n",
    "        X = np.array([\n",
    "            [added_one_seq[position], added_two_seq[position]]\n",
    "        ])  # weź parę liczb od lewej do prawej\n",
    "        hidden_l = hidden_l_values[-position - 1] \n",
    "        prev_hidden_l = hidden_l_values[-position - 2]\n",
    "\n",
    "        # błąd na warstwie wyjściowej\n",
    "        output_l_delta = output_l_deltas[-position - 1]\n",
    "        # błąd na warstwie ukrytej\n",
    "        hidden_l_delta = (future_hidden_l_delta.dot(W.T) + output_l_delta.dot(V.T)) * sigmoid_output_to_derivative(hidden_l)\n",
    "\n",
    "        # aktualizacja macierzy poprawek względem błędu w aktualnym kroku (na aktualnej pozycji w sekwencji)\n",
    "        V_update += np.atleast_2d(hidden_l).T.dot(output_l_delta)\n",
    "        W_update += np.atleast_2d(prev_hidden_l).T.dot(hidden_l_delta)\n",
    "        U_update += X.T.dot(hidden_l_delta)\n",
    "\n",
    "        future_hidden_l_delta = hidden_l_delta\n",
    "\n",
    "    U -= U_update * alpha  # spadek wag między wejściem a w. ukrytą\n",
    "    V -= V_update * alpha  # spadek wag między w. ukrytą a wyjściem\n",
    "    W -= W_update * alpha  # spadek wag między poprzednią a teraźniejszą w. ukrytą\n",
    "\n",
    "    U_update *= 0  # zerowanie macierz\n",
    "    V_update *= 0  # zerowanie macierz\n",
    "    W_update *= 0  # zerowanie macierz\n",
    "\n",
    "    if(j % 5000 == 0):\n",
    "        print(\"Błąd przykładu:\" + str(overallError))\n",
    "        print(\"Przewidziana sekwencja:\" + str(predicted_sum_seq))\n",
    "        print(\"Oczekiwana sekwencja:  \" + str(expected_sum_seq))\n",
    "        print(\"------------\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
