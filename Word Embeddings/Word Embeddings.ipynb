{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Miara cosinusowa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.7977198918178166\n",
      "0.23409628697705598\n",
      "-0.48434715333575534\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np            \n",
    "def cosine(vec1, vec2):\n",
    "    top = 0\n",
    "    bot_a = 0\n",
    "    bot_b = 0\n",
    "    for i in range(len(vec1)):\n",
    "        top += vec1[i] * vec2[i]\n",
    "        bot_a += vec1[i] * vec1[i]\n",
    "        bot_b += vec2[i] * vec2[i]\n",
    "    bot = np.sqrt(bot_a) * np.sqrt(bot_b)\n",
    "    return top/bot\n",
    "\n",
    "\n",
    "print(cosine([1.0, 2.0, 3.0], [1.5, -0.7, -20]))\n",
    "print(cosine([-10.0, 17.0, 2.0], [5.3, 12.0, -20]))\n",
    "print(cosine([1.0, 2.0, 3.0], [1, -3000, 184]))\n",
    "print(cosine([1.0, 2.0, 3.0], [1, 2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wykorzystanie BoW do porównania dwóch podobnych tekstów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokumenty reprezentowane jako BOW\n",
      "[[1 1 1 0 0]\n",
      " [1 1 1 1 1]]\n",
      "\n",
      "Podobieńśtwo dokumentów to\n",
      "0.7745966692414834\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "doc1 = \"Ala ma kota\"\n",
    "doc2 = \"Ala ma pięknego puszystego kota\"\n",
    "\n",
    "docs = [doc1, doc2]\n",
    "\n",
    "X_train_counts = count_vect.fit_transform(docs).todense()\n",
    "\n",
    "print(\"Dokumenty reprezentowane jako BOW\") # dokumenty w wierszach, słowa w kolumnach\n",
    "print(X_train_counts)\n",
    "print(\"\\nPodobieńśtwo dokumentów to\")\n",
    "print(cosine(X_train_counts[0].tolist()[0], X_train_counts[1].tolist()[0])) # zamiana macierzy 1 x n na listę elementów 1 x n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wykorzystanie BoW do porównania synonimów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokumenty reprezentowane jako BOW\n",
      "[[1 0]\n",
      " [0 1]]\n",
      "\n",
      "\n",
      "Podobieńśtwo dokumentów to\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "doc1 = \"kot\"\n",
    "doc2 = \"kotek\"\n",
    "docs = [doc1, doc2]\n",
    "\n",
    "\n",
    "print(\"Dokumenty reprezentowane jako BOW\") # dokumenty w wierszach, słowa w kolumnach\n",
    "X_train_counts = count_vect.fit_transform(docs).todense()\n",
    "print(X_train_counts)\n",
    "\n",
    "print(\"\\n\\nPodobieńśtwo dokumentów to\")\n",
    "print(cosine(X_train_counts[0].tolist()[0], X_train_counts[1].tolist()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding przykład"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Podobieńśtwo między kotem a kotkiem:\n",
      "0.6386305647068644\n",
      "Podobieńśtwo między kotem a krzesłem\n",
      "0.2942529771662456\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_embeddings(path):\n",
    "    mapping = dict()\n",
    "    \n",
    "    with open(path, 'r', encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if len(line) == 0:\n",
    "                continue\n",
    "            splitted = line.split(\" \")\n",
    "            mapping[splitted[0]] = np.array(splitted[1:], dtype=float) # stwórz słownik słowo -> wektor \n",
    "    return mapping\n",
    "    \n",
    "# https://www.kaggle.com/datasets/watts2/glove6b50dtxt link do pliku\n",
    "mapping = load_embeddings('glove.6B.50d.txt') # załadowanie wektora o długości 50 liczb\n",
    "\n",
    "kot = mapping['cat']\n",
    "kotek = mapping['kitten']\n",
    "krzeslo = mapping['chair']\n",
    "\n",
    "print(\"Podobieńśtwo między kotem a kotkiem:\")\n",
    "print(cosine(kot, kotek))\n",
    "\n",
    "print(\"Podobieńśtwo między kotem a krzesłem\")\n",
    "print(cosine(kot, krzeslo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relacje między wektorami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poland\n"
     ]
    }
   ],
   "source": [
    "def get_most_similar(vec1, embeddings): # funkcja zwracająca słowo leżące najbliżej danego wektora\n",
    "    slowo = \"\"\n",
    "    score = 0\n",
    "    for word, vector in embeddings.items():\n",
    "        if cosine(vec1, vector) > score:\n",
    "            slowo = word\n",
    "            score = cosine(vec1, vector)\n",
    "    return slowo\n",
    "\n",
    "new_point = mapping['italy'] - mapping['rome'] + mapping['warsaw']\n",
    "print(get_most_similar(new_point, mapping))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wykorzystanie embeddingów w klasyfikacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1624, 50)\n",
      "W zbiorze testowym poprawnie zaklasyfikowanych zostało 91.81446111869032% przypadków\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94       517\n",
      "           1       0.88      0.84      0.86       216\n",
      "\n",
      "    accuracy                           0.92       733\n",
      "   macro avg       0.91      0.89      0.90       733\n",
      "weighted avg       0.92      0.92      0.92       733\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "full_dataset = pandas.read_csv('spam_emails.csv', encoding='utf-8')      # wczytanie danych z pliku CSV\n",
    "full_dataset['label_num'] = full_dataset.label.map({'ham':0, 'spam':1})  # mapowanie wartości na liczby \n",
    "\n",
    "np.random.seed(0)                                       # ustawienie seedu na 0\n",
    "train_indices = np.random.rand(len(full_dataset)) < 0.7 # wylosowanie 70% wierszy, które znajdą się w zbiorze treningowym\n",
    "\n",
    "train = full_dataset[train_indices] # wybór zbioru treningowego\n",
    "test = full_dataset[~train_indices] # wybór zbioru testowego\n",
    "\n",
    "\n",
    "def avg_embedding(doc, embedding):\n",
    "    array = [embedding[token] for token in word_tokenize(doc.lower()) if token in embedding]\n",
    "    return np.mean(array, axis=0)\n",
    "\n",
    "def documents_to_ave_embeddings(docs, emb):\n",
    "    return np.array([avg_embedding(doc, emb) for doc in docs])\n",
    "    \n",
    " \n",
    "classifier = SVC(C=1.0)\n",
    "\n",
    "train_transformed = documents_to_ave_embeddings(train['text'], mapping)\n",
    "print(train_transformed.shape)\n",
    "test_transformed = documents_to_ave_embeddings(test['text'], mapping)\n",
    "\n",
    "\n",
    "classifier.fit(train_transformed, train['label_num']) # wektoryzacja danych i trening klasyfikatora\n",
    "\n",
    "# ------------------- OCENA KLASYFIKATORA -----------\n",
    "accuracy = classifier.score(test_transformed, test['label_num'])\n",
    "print(f\"W zbiorze testowym poprawnie zaklasyfikowanych zostało {100*accuracy}% przypadków\")\n",
    "print(classification_report(test['label_num'], classifier.predict(test_transformed))) # testowanie klasyfikatora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wykorzystanie odległości Levensteina oraz embeddingów do poprawienia literówek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tekst oryginalny: Czerny kąt z bialym psem chodza rasem po dahu.\n",
      "Tekst poprawiony: czarny kot za białym psem chodzą razem po dachu .\n",
      "Czas poprawiania: 1.0332718999998178.\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics.distance import edit_distance\n",
    "from nltk import word_tokenize\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "input_text = \"Czerny kąt z bialym psem chodza rasem po dahu.\"\n",
    "\n",
    "with open('slowa_min.txt', 'r', encoding='utf8') as f:\n",
    "    valid_words = set([w.strip() for w in f.read().split(\"\\n\") if w.strip() != '']) # zbiór poprawnych słów\n",
    "\n",
    "def get_closest_word(token, valid_words):\n",
    "    slowo = ''\n",
    "    score = 100\n",
    "    for word in valid_words:\n",
    "        if edit_distance(token, word) < score:\n",
    "            slowo = word\n",
    "            score = edit_distance(token, word)\n",
    "    return slowo\n",
    "        \n",
    "\n",
    "tokenized_input = word_tokenize(input_text) # tokenizacja\n",
    "start = timer()\n",
    "corrected = []   # lista poprawionych tokenów\n",
    "for token in tokenized_input:\n",
    "    if not token.isalpha() or token in valid_words: # jeśli token nie jest słowem lub jest już w słowniku - nie poprawiaj\n",
    "        corrected.append(token)\n",
    "    else: # poprawianie\n",
    "        corrected.append(get_closest_word(token, valid_words))\n",
    "end = timer()\n",
    "\n",
    "\n",
    "print(\"Tekst oryginalny: {t}\".format(t=input_text))\n",
    "print(\"Tekst poprawiony: {t}\".format(t=\" \".join(corrected)))\n",
    "print(\"Czas poprawiania: {t}.\".format(t=end-start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
